# -*- coding: utf-8 -*-
"""SarcasmDetection_Glove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l1zAb1R5P4tWRvaELH8bk7TBcXIzy4Ta
"""

import zipfile

# Unzip the downloaded file
zip_ref = zipfile.ZipFile("/content/archive (35).zip", "r")
zip_ref.extractall()
zip_ref.close()

# required libraries
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix
import warnings
warnings.filterwarnings('ignore')

import pandas as pd

df = pd.read_json('/content/Sarcasm_Headlines_Dataset.json', lines=True)
df.head()

# read data
def parse_data(file):
    for l in open(file,'r'):
        yield json.loads(l)

data = list(parse_data('/content/Sarcasm_Headlines_Dataset.json'))

# convert to pandas dataframe
df = pd.DataFrame(data)
df.head()

df.info()

df = df[['is_sarcastic', 'headline']]
df.head()

sns.set_theme(style="whitegrid")
sns.countplot(x = 'is_sarcastic', data = df);

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,5))
text_len=df[df['is_sarcastic']==1]['headline'].drop_duplicates().apply(len)
ax1.hist(text_len,color='lightblue')
ax1.set_title('Sarcastic text length')
text_len=df[df['is_sarcastic']==0]['headline'].drop_duplicates().apply(len)
ax2.hist(text_len,color='lightgreen')
ax2.set_title('Not Sarcastic text length');

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,5))
text_len=df[df['is_sarcastic']==1]['headline'].drop_duplicates().str.split().map(lambda x: len(x))
ax1.hist(text_len,color='tomato')
ax1.set_title('Sarcastic text word count')
text_len=df[df['is_sarcastic']==0]['headline'].drop_duplicates().str.split().map(lambda x: len(x))
ax2.hist(text_len,color='lightblue')
ax2.set_title('Not Sarcastic text word count');

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,6))
word=df[df['is_sarcastic']==1]['headline'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
ax1.set_title('Sarcastic average word length')
word=df[df['is_sarcastic']==0]['headline'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Not Sarcastic average word length');

labels = np.array(df.is_sarcastic)
sentences = np.array(df.headline)
print('Number of sentences and labels: ', len(labels), len(sentences))

x_train, x_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2)
print('Train and Test set distribution: ', len(x_train), len(x_test), len(y_train), len(y_test))

vocab_size = 10000
max_length = 32
embedding_dim = 32
padding_type='post'
oov_token = '<OOV>'

# tokinizing the texts
tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)
tokenizer.fit_on_texts(x_train)
word_index = tokenizer.word_index
#print(wored_index)

# pdding
train_sequences = tokenizer.texts_to_sequences(x_train)
padded_train_sequences = pad_sequences(train_sequences, maxlen = max_length, padding = padding_type)

test_sequences = tokenizer.texts_to_sequences(x_test)
padded_test_sentences = pad_sequences(test_sequences, maxlen = max_length, padding = padding_type)

number_of_epochs = 10
lstm1_dim = 64
lstm2_dim = 32
gru_dim = 32
filters = 128
kernel_size = 5
lr = 0.0001

# model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(128, activation = 'relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation = 'relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation = 'sigmoid'),
])

model.summary()

model.compile(
    loss = 'binary_crossentropy',
    optimizer = tf.keras.optimizers.Adam(learning_rate = lr),
    metrics = ['accuracy']
)

# raining
history = model.fit(padded_train_sequences, y_train, epochs = number_of_epochs, validation_data=(padded_test_sentences, y_test), verbose=1)

# Plot utility
def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()
    
# Plot the accuracy and loss
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

print('Accuracy on test set: ', model.evaluate(padded_test_sentences,y_test)[1]*100)

preds = model.predict(padded_test_sentences).flatten()
y_predict = []
for i in preds:
    if i < 0.5:
        y_predict.append(0)
    else:
        y_predict.append(1)
y_predict[:5]

print(classification_report(y_test, y_predict, target_names = ['Not Sarcastic','Sarcastic']))

cm = confusion_matrix(y_test,y_predict)
print(cm)

sns.heatmap(cm, square=True, annot=True, cmap='Blues', fmt='d', cbar=False);

import requests
import zipfile
import io

# Download GloVe embeddings
url = "http://nlp.stanford.edu/data/glove.6B.zip"
response = requests.get(url)
compressed_file = io.BytesIO(response.content)
zip_ref = zipfile.ZipFile(compressed_file)
zip_ref.extractall()

# The rest of the code remains the same

# Prepare GloVe embedding matrix
def create_embedding_matrix(filepath, word_index, embedding_dim):
    vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    with open(filepath, encoding="utf-8") as f:
        for line in f:
            word, *vector = line.split()
            if word in word_index:
                idx = word_index[word]
                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]

    return embedding_matrix

embedding_dim = 100
embedding_matrix = create_embedding_matrix('glove.6B.100d.txt', tokenizer.word_index, embedding_dim)

#... (previous code remains the same)

vocab_size = len(tokenizer.word_index) + 1  # Update vocab_size

#... (previous code remains the same)

# Model
model1 = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

#... (the rest of the code remains the same)

model1.compile(
    loss = 'binary_crossentropy',
    optimizer = tf.keras.optimizers.Adam(learning_rate = lr),
    metrics = ['accuracy']
)

# raining
history1 = model1.fit(padded_train_sequences, y_train, epochs = 50, validation_data=(padded_test_sentences, y_test), verbose=1)

# Plot utility
def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()
    
# Plot the accuracy and loss
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

# Plot utility
def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()
    
# Plot the accuracy and loss
plot_graphs(history1, "accuracy")
plot_graphs(history1, "loss")

import random

def random_prediction(model, x_test, y_test, tokenizer):
    index = random.randint(0, len(x_test) - 1)
    test_data = x_test[index]

    # Tokenize and pad the test data
    test_sequence = tokenizer.texts_to_sequences([test_data])
    padded_test_sequence = pad_sequences(test_sequence, maxlen=max_length, padding=padding_type)

    # Make a prediction
    prediction = model.predict(padded_test_sequence)
    predicted_label = 1 if prediction >= 0.5 else 0

    print("Test Data: ", test_data)
    print("Actual Label: ", y_test[index])
    print("Predicted Label: ", predicted_label)

# Call the function with your model, x_test, y_test, and tokenizer
random_prediction(model1, x_test, y_test, tokenizer)

#... (previous code remains the same)


#... (the rest of the code remains the same)
vocab_size = len(tokenizer.word_index) + 1  # Update vocab_size

#... (previous code remains the same)

# Model
model1 = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

vocab_size = len(tokenizer.word_index) + 1  # Update vocab_size

# Prepare GloVe embedding matrix
def create_embedding_matrix(filepath, word_index, embedding_dim):
    vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    with open(filepath, encoding="utf-8") as f:
        for line in f:
            word, *vector = line.split()
            if word in word_index:
                idx = word_index[word]
                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]

    return embedding_matrix

embedding_matrix = create_embedding_matrix('glove.6B.200d.txt', tokenizer.word_index, embedding_dim)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

embedding_dim = 200  # Increase embedding dimension size
lstm3_dim = 16  # Add an additional LSTM layer

# Model
model2 = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim, return_sequences=True)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm3_dim)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model2.compile(
    loss = 'binary_crossentropy',
    optimizer = tf.keras.optimizers.Adam(learning_rate = lr),
    metrics = ['accuracy']
)

# Set up Early Stopping and Model Checkpoint callbacks
early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)

# Training
history2 = model2.fit(padded_train_sequences, y_train, epochs=150, validation_data=(padded_test_sentences, y_test), verbose=1, callbacks=[early_stopping, model_checkpoint])

# Plot utility
def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()
    
# Plot the accuracy and loss
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

from tensorflow.keras.models import load_model
best_model = load_model('best_model.h5')
test_loss, test_accuracy = best_model.evaluate(padded_test_sentences, y_test)
print("Test accuracy: {:.2f}%".format(test_accuracy * 100))

import random

def random_prediction(model, x_test, y_test, tokenizer):
    index = random.randint(0, len(x_test) - 1)
    test_data = x_test[index]

    # Tokenize and pad the test data
    test_sequence = tokenizer.texts_to_sequences([test_data])
    padded_test_sequence = pad_sequences(test_sequence, maxlen=max_length, padding=padding_type)

    # Make a prediction
    prediction = model.predict(padded_test_sequence)
    predicted_label = 1 if prediction >= 0.5 else 0

    print("Test Data: ", test_data)
    print("Actual Label: ", y_test[index])
    print("Predicted Label: ", predicted_label)

# Call the function with your model, x_test, y_test, and tokenizer
random_prediction(best_model, x_test, y_test, tokenizer)